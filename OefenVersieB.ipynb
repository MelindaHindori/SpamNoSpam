{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Import packages\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--dirty--\n",
      "word         frequency\n",
      "-------------------------\n",
      "enron             9432\n",
      "ect               8033\n",
      "vinc              5908\n",
      "subject           4034\n",
      "hou               3942\n",
      "com               3926\n",
      "pleas             3554\n",
      "kaminski          3306\n",
      "would             3064\n",
      "cc                2717\n",
      "thank             2601\n",
      "j                 2600\n",
      "pm                2251\n",
      "forward           2217\n",
      "time              2212\n",
      "research          2001\n",
      "price             1984\n",
      "market            1967\n",
      "inform            1953\n",
      "work              1863\n",
      "know              1849\n",
      "group             1839\n",
      "manag             1784\n",
      "get               1759\n",
      "may               1704\n",
      "meet              1688\n",
      "use               1668\n",
      "busi              1656\n",
      "like              1622\n",
      "e                 1620\n",
      "mail              1610\n",
      "new               1600\n",
      "model             1598\n",
      "need              1591\n",
      "risk              1584\n",
      "energi            1584\n",
      "compani           1548\n",
      "one               1444\n",
      "regard            1413\n",
      "look              1404\n",
      "messag            1403\n",
      "let               1386\n",
      "power             1350\n",
      "make              1340\n",
      "email             1332\n",
      "develop           1327\n",
      "year              1319\n",
      "us                1292\n",
      "also              1281\n",
      "interest          1268\n",
      "day               1258\n",
      "corp              1196\n",
      "http              1194\n",
      "shirley           1184\n",
      "week              1175\n",
      "call              1161\n",
      "edu               1158\n",
      "houston           1100\n",
      "contact           1087\n",
      "receiv            1083\n",
      "option            1083\n",
      "follow            1080\n",
      "see               1079\n",
      "present           1067\n",
      "commun            1042\n",
      "want              1029\n",
      "provid            1005\n",
      "help               999\n",
      "could              987\n",
      "trade              982\n",
      "program            978\n",
      "project            974\n",
      "confer             969\n",
      "request            967\n",
      "discuss            966\n",
      "www                963\n",
      "list               961\n",
      "interview          950\n",
      "send               940\n",
      "take               923\n",
      "go                 916\n",
      "servic             915\n",
      "th                 914\n",
      "best               896\n",
      "includ             884\n",
      "product            871\n",
      "free               851\n",
      "think              844\n",
      "date               830\n",
      "report             828\n",
      "order              825\n",
      "avail              816\n",
      "number             813\n",
      "attach             810\n",
      "well               795\n",
      "next               795\n",
      "financ             791\n",
      "sent               788\n",
      "offer              787\n",
      "question           785\n",
      "good               782\n",
      "crenshaw           782\n",
      "address            779\n",
      "system             772\n",
      "ga                 772\n",
      "name               769\n",
      "posit              763\n",
      "visit              760\n",
      "give               748\n",
      "schedul            735\n",
      "offic              734\n",
      "chang              732\n",
      "univers            730\n",
      "fax                724\n",
      "john               723\n",
      "stinson            717\n",
      "last               696\n",
      "review             695\n",
      "talk               695\n",
      "current            693\n",
      "phone              690\n",
      "process            682\n",
      "industri           681\n",
      "peopl              669\n",
      "state              667\n",
      "team               666\n",
      "p                  658\n",
      "two                657\n",
      "ee                 657\n",
      "data               654\n",
      "credit             653\n",
      "issu               653\n",
      "opportun           651\n",
      "rice               649\n",
      "site               642\n",
      "come               642\n",
      "back               641\n",
      "invest             630\n",
      "x                  630\n",
      "resum              630\n",
      "plan               628\n",
      "first              627\n",
      "financi            625\n",
      "much               621\n",
      "way                619\n",
      "start              619\n",
      "gener              619\n",
      "find               615\n",
      "origin             611\n",
      "shall              595\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Import data needed\n",
    "df = pd.read_csv('./emails.train.csv')\n",
    "#df_pos = df[df['spam']==1]\n",
    "\n",
    "# 3. Start to clean the data \n",
    "#text = np.random.choice(df['text'])\n",
    "#path = os.getcwd()\n",
    "#os.chdir(path)\n",
    "#train_df = pd.read_csv('./emails.train.csv', nrows=20, delimiter=',')\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+') #tokens without special character\n",
    "stop_words = set(stopwords.words('english')) #stop words remover\n",
    "ps = PorterStemmer() #stemming words\n",
    "lemmatizer = WordNetLemmatizer() #lemmatizing words\n",
    "\n",
    "del_words = ['subject', '_']\n",
    "\n",
    "print('--dirty--')\n",
    "#print(text)\n",
    "\n",
    "#print(stop_words)\n",
    "\n",
    "def select_vocabulary(dataframe, topN=1000):\n",
    "    #for w in dataframe.str.lower():\n",
    "    word2freq = dict()\n",
    "    for line in dataframe:\n",
    "        line = re.sub(\"(^|\\W)\\d+($|\\W)\", \" \", line)\n",
    "        word_tokens = tokenizer.tokenize(line) # tokenize all the words in line\n",
    "        #words = line.split()\n",
    "        for word in word_tokens: \n",
    "            if word not in stop_words:\n",
    "                if word not in del_words: # we check if the current words were looking at are not in the stopwords, we wnat to use as ft\n",
    "                    #if word != 'Subject':\n",
    "                        word = ps.stem(word) # only for the words that I am going to use, with \"word\"we overwrite waht used to be the word that is stemmed\n",
    "                        word2freq[word] = word2freq.setdefault(word, 0) +1\n",
    "\n",
    "    word_freq = [ (word,freq) for word,freq in word2freq.items() ]\n",
    "    \n",
    "    # sort according to freq, in descending order.\n",
    "    word_freq.sort(key=lambda x:x[1], reverse=True) \n",
    "    \n",
    "    # show selection results\n",
    "    print(\"%-10s  %10s\" % ('word', 'frequency'))\n",
    "    print(\"-------------------------\")\n",
    "    for i in range(150):\n",
    "        print(\"%-10s  %10d\" % (word_freq[i]))\n",
    "    print(\"...\\n\")\n",
    "    return([x[0] for x in word_freq[:topN]])\n",
    "\n",
    "vocabulary = select_vocabulary(df['text'])\n",
    "#print(vocabulary)\n",
    "word2ind   = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "#print(word2ind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting feature for train ...\n",
      "   0 / 4021 \n",
      "1000 / 4021 \n",
      "2000 / 4021 \n",
      "3000 / 4021 \n",
      "4000 / 4021 \n",
      "4020 / 4021 \n",
      "[[  0.   0.   0. ...,   0.   0.   0.]\n",
      " [  0.   0.   0. ...,   0.   0.   0.]\n",
      " [  0.   0.   0. ...,   0.   0.   0.]\n",
      " ..., \n",
      " [  4.   4.   2. ...,   0.   0.   0.]\n",
      " [  0.  11.   7. ...,   0.   0.   0.]\n",
      " [  0.   0.   0. ...,   0.   0.   0.]]\n",
      "Extracting feature for test ...\n",
      "   0 / 1707 \n",
      "1000 / 1707 \n",
      "1706 / 1707 \n",
      "Finish.\n"
     ]
    }
   ],
   "source": [
    "def extract_Bag_of_Word_feature(dataframe):\n",
    "    BoWs = np.zeros((len(dataframe), len(vocabulary)), dtype=np.float32)\n",
    "\n",
    "    for i, line in enumerate(dataframe):\n",
    "        for word in line.split():   ## these are the not stemmed words\n",
    "            word = ps.stem(word)    # looked for the stemmed words in our dict cause we stemmed them \n",
    "            word_ind = word2ind.get(word, -1) #looks in dict\n",
    "            if(word_ind>=0):\n",
    "                BoWs[i, word_ind] += 1\n",
    "\n",
    "        if i%1000==0:\n",
    "            print(\"%4d / %d \" % (i, len(dataframe)))\n",
    "    print(\"%4d / %d \" % (i, len(dataframe)))\n",
    "            \n",
    "    return BoWs\n",
    "\n",
    "\n",
    "# Make sure use cleaned version\n",
    "train = pd.read_csv('./emails.train.csv')\n",
    "test  = pd.read_csv('./emails.test.csv')\n",
    "\n",
    "# Get labels\n",
    "Y_train = train['spam']\n",
    "Y_test  = test['spam']\n",
    "\n",
    "print(\"Extracting feature for train ...\")\n",
    "X_train = extract_Bag_of_Word_feature(train['text']) # these are our features\n",
    "print(X_train)\n",
    "\n",
    "print(\"Extracting feature for test ...\")\n",
    "X_test  = extract_Bag_of_Word_feature(test[ 'text']) # this is prediction \n",
    "\n",
    "print('Finish.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Performance of: SVM =======\n",
      "Metric[Accuracy            ]  0.967194\n",
      "Metric[Average Precision   ]  0.983188\n",
      "[output] solution.SVM.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC()\n",
    "\n",
    "model.fit(X_train, Y_train) # fit is for our training data, the extracted bag of words\n",
    "\n",
    "def eval(model, X_test, Y_test, method=''):\n",
    "    print(\"====== Performance of: {method} =======\".format(method=method))\n",
    "    \n",
    "    # Predict decision labels.\n",
    "    Y_pred  = model.predict(X_test)  #we need this \n",
    "    print(\"Metric[{metric:20s}]  {score:-3f}\".format( metric=\"Accuracy\", \n",
    "                                              score=accuracy_score(Y_test, Y_pred)) )\n",
    "\n",
    "    # Predict confidence scores.\n",
    "    Y_score = model.decision_function(X_test)    \n",
    "    print(\"Metric[{metric:20s}]  {score:-3f}\".format( metric=\"Average Precision\", \n",
    "                                              score=average_precision_score(Y_test, Y_score)) )\n",
    "\n",
    "    # write to submit format\n",
    "    outf = 'solution.%s.csv'% method\n",
    "    with open( outf, 'w') as f:\n",
    "        f.write('id,spam\\n')\n",
    "        for i in range(len(Y_pred)):\n",
    "            # print test['id'][0]\n",
    "            f.write('%s,%s\\n' % (test['id'][i], Y_pred[i]) )\n",
    "    print(\"[output] \"+outf)\n",
    "    \n",
    "    \n",
    "# evaluate current model\n",
    "eval(model, X_test, Y_test, method='SVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
